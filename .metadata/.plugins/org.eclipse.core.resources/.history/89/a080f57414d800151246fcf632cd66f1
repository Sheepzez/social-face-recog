package isaacjordan.me.FaceRecog;

import static org.bytedeco.javacpp.opencv_core.CV_32SC1;
import static org.bytedeco.javacpp.opencv_core.FONT_HERSHEY_PLAIN;
import static org.bytedeco.javacpp.opencv_core.IPL_DEPTH_8U;
import static org.bytedeco.javacpp.opencv_core.cvLoad;
import static org.bytedeco.javacpp.opencv_face.createLBPHFaceRecognizer;
import static org.bytedeco.javacpp.opencv_imgcodecs.CV_LOAD_IMAGE_GRAYSCALE;
import static org.bytedeco.javacpp.opencv_imgcodecs.imread;
import static org.bytedeco.javacpp.opencv_imgproc.COLOR_BGRA2GRAY;
import static org.bytedeco.javacpp.opencv_imgproc.cvtColor;
import static org.bytedeco.javacpp.opencv_imgproc.equalizeHist;
import static org.bytedeco.javacpp.opencv_imgproc.putText;
import static org.bytedeco.javacpp.opencv_imgproc.rectangle;

import java.io.File;
import java.io.FilenameFilter;
import java.io.IOException;
import java.net.MalformedURLException;
import java.net.URL;
import java.nio.IntBuffer;
import java.util.HashMap;
import java.util.Map;

import org.bytedeco.javacpp.Loader;
import org.bytedeco.javacpp.opencv_core.CvMemStorage;
import org.bytedeco.javacpp.opencv_core.IplImage;
import org.bytedeco.javacpp.opencv_core.Mat;
import org.bytedeco.javacpp.opencv_core.MatVector;
import org.bytedeco.javacpp.opencv_core.Point;
import org.bytedeco.javacpp.opencv_core.Rect;
import org.bytedeco.javacpp.opencv_core.RectVector;
import org.bytedeco.javacpp.opencv_core.Scalar;
import org.bytedeco.javacpp.opencv_face.FaceRecognizer;
import org.bytedeco.javacpp.opencv_objdetect;
import org.bytedeco.javacpp.opencv_objdetect.CascadeClassifier;
import org.bytedeco.javacpp.opencv_objdetect.CvHaarClassifierCascade;
import org.bytedeco.javacv.CanvasFrame;
import org.bytedeco.javacv.Frame;
import org.bytedeco.javacv.FrameGrabber;
import org.bytedeco.javacv.FrameGrabber.Exception;
import org.bytedeco.javacv.OpenCVFrameConverter;

public class GrabberShow implements Runnable {
	final int INTERVAL = 50;
	IplImage image;
	FaceRecognizer faceRecognizer;
	Map<Integer, String> idToName;

	public GrabberShow() {
		String trainingDir = "images";
		File root = new File(trainingDir);

		FilenameFilter imgFilter = new FilenameFilter() {
			public boolean accept(File dir, String name) {
				name = name.toLowerCase();
				return name.endsWith(".jpg") || name.endsWith(".pgm") || name.endsWith(".png");
			}
		};

		File[] imageFiles = root.listFiles(imgFilter);

		MatVector images = new MatVector(imageFiles.length);

		Mat labels = new Mat(imageFiles.length, 1, CV_32SC1);
		IntBuffer labelsBuf = labels.getIntBuffer();

		int counter = 0;
		idToName = new HashMap<Integer, String>();

		for (File image : imageFiles) {
			Mat img = imread(image.getAbsolutePath(), CV_LOAD_IMAGE_GRAYSCALE);
			String[] props = image.getName().split("-");
			int label = Integer.parseInt(props[0]);

			idToName.put(label, props[1]);

			images.put(counter, img);

			labelsBuf.put(counter, label);

			counter++;
		}

		// faceRecognizer = createFisherFaceRecognizer();
		// faceRecognizer = createEigenFaceRecognizer();
		faceRecognizer = createLBPHFaceRecognizer();

		System.out.println("Training face rocognizer.");
		faceRecognizer.train(images, labels);
		System.out.println("Finished training.");

	}

	public void run() {
		String classifierName = null;
		URL url = null;
		try {
			url = new URL(
					"https://raw.github.com/Itseez/opencv/2.4.0/data/haarcascades/haarcascade_frontalface_alt.xml");
		} catch (MalformedURLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		File file = null;
		try {
			file = Loader.extractResource(url, null, "classifier", ".xml");
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		file.deleteOnExit();
		classifierName = file.getAbsolutePath();

		// Preload the opencv_objdetect module to work around a known bug.
		Loader.load(opencv_objdetect.class);

		// We can "cast" Pointer objects by instantiating a new object of the
		// desired class.
		CvHaarClassifierCascade classifier = new CvHaarClassifierCascade(cvLoad(classifierName));
		// CvHaarClassifierCascade face_cascade = new
		// CvHaarClassifierCascade(cvLoad(classifierName));
		if (classifier.isNull()) {
			System.err.println("Error loading classifier file \"" + classifierName + "\".");
			System.exit(1);
		}

		// The available FrameGrabber classes include OpenCVFrameGrabber
		// (opencv_videoio),
		// DC1394FrameGrabber, FlyCaptureFrameGrabber, OpenKinectFrameGrabber,
		// PS3EyeFrameGrabber, VideoInputFrameGrabber, and FFmpegFrameGrabber.
		FrameGrabber grabber = null;
		try {
			grabber = FrameGrabber.createDefault(0);
			grabber.start();
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

		// CanvasFrame, FrameGrabber, and FrameRecorder use Frame objects to
		// communicate image data.
		// We need a FrameConverter to interface with other APIs (Android, Java
		// 2D, or OpenCV).
		OpenCVFrameConverter.ToIplImage converter = new OpenCVFrameConverter.ToIplImage();

		// FAQ about IplImage and Mat objects from OpenCV:
		// - For custom raw processing of data, createBuffer() returns an NIO
		// direct
		// buffer wrapped around the memory pointed by imageData, and under
		// Android we can
		// also use that Buffer with Bitmap.copyPixelsFromBuffer() and
		// copyPixelsToBuffer().
		// - To get a BufferedImage from an IplImage, or vice versa, we can
		// chain calls to
		// Java2DFrameConverter and OpenCVFrameConverter, one after the other.
		// - Java2DFrameConverter also has static copy() methods that we can use
		// to transfer
		// data more directly between BufferedImage and IplImage or Mat via
		// Frame objects.
		IplImage grabbedImage = null;
		try {
			grabbedImage = converter.convert(grabber.grab());
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

		// CanvasFrame is a JFrame containing a Canvas component, which is
		// hardware accelerated.
		// It can also switch into full-screen mode when called with a
		// screenNumber.
		// We should also specify the relative monitor/camera response for
		// proper gamma correction.
		CanvasFrame frame = new CanvasFrame("Face File Generator", CanvasFrame.getDefaultGamma() / grabber.getGamma());

		Frame videoFrame;
		Mat videoMat = new Mat();
		OpenCVFrameConverter.ToMat converterToMat = new OpenCVFrameConverter.ToMat();
		CascadeClassifier face_cascade = new CascadeClassifier("data/haarcascades/haarcascade_frontalface_default.xml");
		if (face_cascade.isNull()) {
			System.err.println("Error loading classifier file \"" + classifierName + "\".");
			System.exit(1);
		}

		try {
			while (frame.isVisible() && (videoFrame = grabber.grab()) != null) {

				videoMat = converterToMat.convert(videoFrame);
				Mat videoMatGray = new Mat();
				// Convert the current frame to grayscale:
				cvtColor(videoMat, videoMatGray, COLOR_BGRA2GRAY);
				equalizeHist(videoMatGray, videoMatGray);

				RectVector faces = new RectVector();
				// Find the faces in the frame:
				face_cascade.detectMultiScale(videoMatGray, faces);

				// At this point you have the position of the faces in
				// faces. Now we'll get the faces, make a prediction and
				// annotate it in the video. Cool or what?
				for (int i = 0; i < faces.size(); i++) {
					Rect face_i = faces.get(i);

					Mat face = new Mat(videoMatGray, face_i);
					// If fisher face recognizer is used, the face need to be
					// resized.
					// resize(face, face_resized, new Size(im_width, im_height),
					// 1.0, 1.0, INTER_CUBIC);

					// Now perform the prediction, see how easy that is:
					int prediction = faceRecognizer.predict(face);

					// And finally write all we've found out to the original
					// image!
					// First of all draw a green rectangle around the detected
					// face:
					rectangle(videoMat, face_i, new Scalar(0, 255, 0, 1));

					// Create the text we will annotate the box with:
					String box_text = idToName.get(prediction);
					// Calculate the position for annotated text (make sure we
					// don't
					// put illegal values in there):
					int pos_x = Math.max(face_i.tl().x() - 10, 0);
					int pos_y = Math.max(face_i.tl().y() - 10, 0);
					// And now put it into the image:
					putText(videoMat, box_text, new Point(pos_x, pos_y), FONT_HERSHEY_PLAIN, 1.0,
							new Scalar(0, 255, 0, 2.0));
				}
				// Show the result:
				IplImage image = new IplImage(videoMat);
				Frame convertedFrame = converter.convert(image);
				frame.showImage(convertedFrame);

			}
		} catch (Exception e) {
			e.printStackTrace();
		}

		frame.dispose();

		try {
			grabber.stop();
		} catch (Exception e) {
			e.printStackTrace();
		}

	}
}
